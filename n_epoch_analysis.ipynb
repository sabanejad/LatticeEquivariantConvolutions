{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lRhugY6vYdI",
    "outputId": "57068946-573d-476d-a54f-18f1c3d1e06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION: 1.8.1+cu101\n",
      "TORCH DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "## Code originally in https://arxiv.org/pdf/2101.08176.pdf, slightly adapted for our milestone\n",
    "\n",
    "import base64\n",
    "import io\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import packaging.version\n",
    "%matplotlib inline\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = 'cuda'\n",
    "    float_dtype = np.float32 # single torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "else:\n",
    "    torch_device = 'cpu'\n",
    "    float_dtype = np.float64 # double torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIfA4JCIvhgT"
   },
   "outputs": [],
   "source": [
    "def torch_mod(x):\n",
    "    return torch.remainder(x, 2*np.pi)\n",
    "def torch_wrap(x):\n",
    "    return torch_mod(x+np.pi) - np.pi\n",
    "def grab(var):\n",
    "    return var.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35qjJmVbvrKf"
   },
   "outputs": [],
   "source": [
    "class SimpleNormal:  # prior distribution\n",
    "    def __init__(self, loc, var):\n",
    "        self.dist = torch.distributions.normal.Normal(\n",
    "            torch.flatten(loc), torch.flatten(var))\n",
    "        self.shape = loc.shape\n",
    "    def log_prob(self, x):\n",
    "        logp = self.dist.log_prob(x.reshape(x.shape[0], -1))\n",
    "        return torch.sum(logp, dim=1)\n",
    "    def sample_n(self, batch_size):\n",
    "        x = self.dist.sample((batch_size,))\n",
    "        return x.reshape(batch_size, *self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TpskaK1vtRs"
   },
   "outputs": [],
   "source": [
    "def apply_flow_to_prior(prior, coupling_layers, *, batch_size): # normalizing flow\n",
    "    x = prior.sample_n(batch_size)\n",
    "    logq = prior.log_prob(x)\n",
    "    for layer in coupling_layers:\n",
    "        x, logJ = layer.forward(x)\n",
    "        logq = logq.to(torch_device) - logJ.to(torch_device)\n",
    "    return x.to(torch_device), logq.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wX5lDwekvvI1"
   },
   "outputs": [],
   "source": [
    "class ScalarPhi4Action:  # action for phi^4 2D scalar field theory\n",
    "    def __init__(self, M2, lam):\n",
    "        self.M2 = M2\n",
    "        self.lam = lam\n",
    "    def __call__(self, cfgs):\n",
    "        # potential term\n",
    "        action_density = self.M2*cfgs**2 + self.lam*cfgs**4 # kinetic term (discrete Laplacian)\n",
    "        Nd = len(cfgs.shape)-1\n",
    "        dims = range(1,Nd+1)\n",
    "        for mu in dims:\n",
    "            action_density += 2*cfgs**2\n",
    "            action_density -= cfgs*torch.roll(cfgs, -1, mu)\n",
    "            action_density -= cfgs*torch.roll(cfgs, 1, mu)\n",
    "        return torch.sum(action_density, dim=tuple(dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVqY9vbovwwe"
   },
   "outputs": [],
   "source": [
    "def make_checker_mask(shape, parity):  # checkerboard pattern\n",
    "    checker = torch.ones(shape, dtype=torch.uint8) - parity\n",
    "    checker[::2, ::2] = parity\n",
    "    checker[1::2, 1::2] = parity\n",
    "    return checker.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYr4HmGmvyQp"
   },
   "outputs": [],
   "source": [
    "class AffineCoupling(torch.nn.Module):  # Affine coupling layer\n",
    "    def __init__(self, net, *, mask_shape, mask_parity):\n",
    "        super().__init__()\n",
    "        self.mask = (make_checker_mask(mask_shape, mask_parity)).to(torch_device)\n",
    "        self.net = net.to(torch_device)\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch_device)\n",
    "        x_frozen = (self.mask).to(torch_device) * x\n",
    "        x_active = (1 - self.mask) * x\n",
    "        net_out = self.net(x_frozen.unsqueeze(1))\n",
    "        s, t = net_out[:,0], net_out[:,1]\n",
    "        fx = (1 - self.mask) * t + x_active * torch.exp(s) + x_frozen\n",
    "        axes = range(1,len(s.size()))\n",
    "        logJ = torch.sum((1 - self.mask) * s, dim=tuple(axes))\n",
    "        return fx, logJ\n",
    "    def reverse(self, fx):\n",
    "        fx_frozen = self.mask * fx\n",
    "        fx_active = (1 - self.mask) * fx\n",
    "        net_out = self.net(fx_frozen.unsqueeze(1))\n",
    "        s, t = net_out[:,0], net_out[:,1]\n",
    "        x = (fx_active - (1 - self.mask) * t) * torch.exp(-s) + fx_frozen\n",
    "        axes = range(1,len(s.size()))\n",
    "        logJ = torch.sum((1 - self.mask)*(-s), dim=tuple(axes))\n",
    "        return x, logJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfixgU1vvzxv"
   },
   "outputs": [],
   "source": [
    "def make_conv_net(*, hidden_sizes, kernel_size, in_channels, out_channels, use_final_tanh):   # CNN\n",
    "    sizes = [in_channels] + hidden_sizes + [out_channels]\n",
    "    padding_size = (kernel_size // 2)\n",
    "    net = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        net.append(torch.nn.Conv2d(\n",
    "            sizes[i], sizes[i+1], kernel_size, padding=padding_size, \n",
    "            stride=1, padding_mode='circular'))\n",
    "        if i != len(sizes) - 2:\n",
    "            net.append(torch.nn.LeakyReLU())\n",
    "        else:\n",
    "            if use_final_tanh:\n",
    "                net.append(torch.nn.Tanh())\n",
    "    return torch.nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTqhdakav1z5"
   },
   "outputs": [],
   "source": [
    "def make_phi4_affine_layers(*, n_layers, lattice_shape, hidden_sizes, kernel_size):\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        parity = i % 2\n",
    "        net = make_conv_net(\n",
    "            in_channels=1, out_channels=2, hidden_sizes=hidden_sizes,\n",
    "            kernel_size=kernel_size, use_final_tanh=True)\n",
    "        coupling = AffineCoupling(net, mask_shape=lattice_shape, mask_parity=parity)\n",
    "        layers.append(coupling)\n",
    "    return torch.nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDpe03BRv3lR"
   },
   "outputs": [],
   "source": [
    "def calc_dkl(logp, logq):   # reverse loss\n",
    "    return (logq - logp).mean()\n",
    "def train_step(model, action, loss_fn, optimizer, metrics): \n",
    "    layers, prior = model['layers'], model['prior'] \n",
    "    optimizer.zero_grad()\n",
    "    x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size)\n",
    "    logp = -action(x)\n",
    "    loss = calc_dkl(logp, logq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    metrics['loss'].append(grab(loss)) \n",
    "    metrics['logp'].append(grab(logp)) \n",
    "    metrics['logq'].append(grab(logq))\n",
    "    metrics['ess'].append(grab( compute_ess(logp, logq) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlxQ33A0v5Fb"
   },
   "outputs": [],
   "source": [
    "def compute_ess(logp, logq): # ESS metric\n",
    "    logw = logp - logq\n",
    "    log_ess = 2*torch.logsumexp(logw, dim=0) - torch.logsumexp(2*logw, dim=0)\n",
    "    ess_per_cfg = torch.exp(log_ess) / len(logw)\n",
    "    return ess_per_cfg\n",
    "def print_metrics(history, avg_last_N_epochs): \n",
    "    print(f'== Era {era} | Epoch {epoch} metrics ==') \n",
    "    for key, val in history.items():\n",
    "        avgd = np.mean(val[-avg_last_N_epochs:]) \n",
    "        print(f'\\t{key} {avgd:g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElDgKFSqcAtb"
   },
   "outputs": [],
   "source": [
    "def serial_sample_generator(model, action, batch_size, N_samples): \n",
    "    layers, prior = model['layers'], model['prior']\n",
    "    layers.eval()\n",
    "    x, logq, logp = None, None, None\n",
    "    for i in range(N_samples):\n",
    "        batch_i = i % batch_size\n",
    "        if batch_i == 0:\n",
    "            x, logq = apply_flow_to_prior(prior, layers, batch_size=batch_size) \n",
    "            logp = -action(x)\n",
    "        yield x[batch_i], logq[batch_i], logp[batch_i]\n",
    "\n",
    "def make_mcmc_ensemble(model, action, batch_size, N_samples): # function to generate the enseble where we will test the acceptance rate of our trained model\n",
    "    history = {'x' : [], 'logq' : [], 'logp' : [], 'accepted' : []}\n",
    "    # build Markov chain\n",
    "    sample_gen = serial_sample_generator(model, action, batch_size, N_samples)\n",
    "    for new_x, new_logq, new_logp in sample_gen:\n",
    "        if len(history['logp']) == 0:\n",
    "            accepted = True\n",
    "        else:\n",
    "            last_logp = history['logp'][-1]\n",
    "            last_logq = history['logq'][-1]\n",
    "            p_accept = torch.exp((new_logp - new_logq) - (last_logp - last_logq)) \n",
    "            p_accept = min(1, p_accept)\n",
    "            draw = torch.rand(1).to(torch_device) # ~ [0,1]\n",
    "            if draw < p_accept:\n",
    "                accepted = True\n",
    "            else:\n",
    "                accepted = False\n",
    "                new_x = history['x'][-1]\n",
    "                new_logp = last_logp\n",
    "                new_logq = last_logq\n",
    "        history['logp'].append(new_logp)\n",
    "        history['logq'].append(new_logq)\n",
    "        history['x'].append(new_x) \n",
    "        history['accepted'].append(accepted)\n",
    "    return history\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-q2XpQOgv6-k"
   },
   "outputs": [],
   "source": [
    "# Parameters defined here\n",
    "L = 8  # lattice size\n",
    "lattice_shape = (L,L)\n",
    "M2 = -4.0  # mass\n",
    "lam = 8.0  # coupling constant\n",
    "phi4_action = ScalarPhi4Action(M2=M2, lam=lam)\n",
    "prior = SimpleNormal(torch.zeros(lattice_shape), torch.ones(lattice_shape))\n",
    "n_layers = 16  \n",
    "hidden_sizes = [8,8]  \n",
    "kernel_size = 3 # CNN kernel size\n",
    "layers = make_phi4_affine_layers(\n",
    "    lattice_shape=lattice_shape, n_layers=n_layers,\n",
    "    hidden_sizes=hidden_sizes, kernel_size=kernel_size) \n",
    "model = {'layers': layers, 'prior': prior}\n",
    "base_lr = .001  # learning rate\n",
    "optimizer = torch.optim.Adam(model['layers'].parameters(), lr=base_lr) # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ta1JLvK1wCJR",
    "outputId": "a2380554-478f-415f-dcdc-997febbf0679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 - accept rate: 0.5189208984375\n",
      "120 - accept rate: 0.581787109375\n",
      "130 - accept rate: 0.6044921875\n",
      "140 - accept rate: 0.596923828125\n",
      "150 - accept rate: 0.593505859375\n",
      "160 - accept rate: 0.5836181640625\n",
      "170 - accept rate: 0.6302490234375\n",
      "180 - accept rate: 0.6224365234375\n",
      "190 - accept rate: 0.6031494140625\n",
      "200 - accept rate: 0.6529541015625\n"
     ]
    }
   ],
   "source": [
    "N_epoch_list = np.arange(10, 210, 10)  # test acceptance rate for different n_epoch\n",
    "acceptance_rate = []\n",
    "for N_epoch in N_epoch_list:\n",
    "    N_era = 25\n",
    "    batch_size = 64\n",
    "    print_freq = N_epoch\n",
    "    plot_freq = 1\n",
    "    history = { 'loss' : [],\n",
    "          'logp' : [], 'logq' : [], 'ess' : []\n",
    "          }\n",
    "          \n",
    "    for era in range(N_era):\n",
    "        for epoch in range(N_epoch):\n",
    "            train_step(model, phi4_action, calc_dkl, optimizer, history)\n",
    "\n",
    "    serialized_model = io.BytesIO() \n",
    "    torch.save(model['layers'].state_dict(), serialized_model)\n",
    "\n",
    "    ensemble_size = 8192\n",
    "    phi4_ens = make_mcmc_ensemble(model, phi4_action, 64, ensemble_size) \n",
    "    acceptance_rate.append(np.mean(phi4_ens['accepted']))\n",
    "    print(N_epoch, \"- accept rate:\", np.mean(phi4_ens['accepted']))  #save the output value for each Nepoch you try"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "n_epoch_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
